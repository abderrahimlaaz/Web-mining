{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from gensim import corpora,models,similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['new', 'york', 'times'], ['new', 'york', 'post'], ['los', 'angeles', 'times']]\n"
     ]
    }
   ],
   "source": [
    "documents=[\"new york times\",\"new york post\",\"los angeles times\"]\n",
    "tokenized_documents=[[token for token in document.lower().split()] for document in documents ]\n",
    "pprint(tokenized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(6 unique tokens: ['new', 'times', 'york', 'post', 'angeles']...)\n",
      "\n",
      "\n",
      "number of documents : 3\n"
     ]
    }
   ],
   "source": [
    "dictionary=corpora.Dictionary(tokenized_documents)\n",
    "print(dictionary)\n",
    "print('\\n')\n",
    "print(\"number of documents :\",dictionary.num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'new': 0, 'times': 1, 'york': 2, 'post': 3, 'angeles': 4, 'los': 5}\n",
      "\n",
      " new times\n"
     ]
    }
   ],
   "source": [
    "words_and_indexes=dictionary.token2id\n",
    "\n",
    "print(words_and_indexes)\n",
    "\n",
    "print('\\n', dictionary[0], dictionary[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2bow_vectors of the three documents:[(id_words,tf), (id_word,tf),....,(id_word,tf)]\n",
      "\n",
      "*** The Document number :( 1 )***\n",
      "The word:'new' --- The id_word :0 --- TF:1\n",
      "The word:'times' --- The id_word :1 --- TF:1\n",
      "The word:'york' --- The id_word :2 --- TF:1\n",
      "\n",
      "*** The Document number :( 2 )***\n",
      "The word:'new' --- The id_word :0 --- TF:1\n",
      "The word:'york' --- The id_word :2 --- TF:1\n",
      "The word:'post' --- The id_word :3 --- TF:1\n",
      "\n",
      "*** The Document number :( 3 )***\n",
      "The word:'times' --- The id_word :1 --- TF:1\n",
      "The word:'angeles' --- The id_word :4 --- TF:1\n",
      "The word:'los' --- The id_word :5 --- TF:1\n"
     ]
    }
   ],
   "source": [
    "corpus_doc2bow_vectors=[ dictionary.doc2bow(tok_doc) for tok_doc in tokenized_documents ]\n",
    "print(\"doc2bow_vectors of the three documents:[(id_words,tf), (id_word,tf),....,(id_word,tf)]\")\n",
    "temp=1\n",
    "for c in corpus_doc2bow_vectors :\n",
    "    print(\"\\n*** The Document number :(\",temp,\")***\")\n",
    "    for i in c:\n",
    "        id_word,tf=i\n",
    "        print(f\"The word:\\'{dictionary[id_word]}\\' --- The id_word :{id_word} --- TF:{tf}\")\n",
    "        \n",
    "    temp+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.18 ms\n",
      "----  tfidf_vectors of the three documents:[(id_word,tf), (id_word,tf),....,(id_word,)]  ----\n",
      "\n",
      "*** The Document number :( 1 )***\n",
      "The word:'new' --- The id_word :0 --- Tf_Idf: 0.5849625007211562\n",
      "The word:'times' --- The id_word :1 --- Tf_Idf: 0.5849625007211562\n",
      "The word:'york' --- The id_word :2 --- Tf_Idf: 0.5849625007211562\n",
      "\n",
      "*** The Document number :( 2 )***\n",
      "The word:'new' --- The id_word :0 --- Tf_Idf: 0.5849625007211562\n",
      "The word:'york' --- The id_word :2 --- Tf_Idf: 0.5849625007211562\n",
      "The word:'post' --- The id_word :3 --- Tf_Idf: 1.5849625007211563\n",
      "\n",
      "*** The Document number :( 3 )***\n",
      "The word:'times' --- The id_word :1 --- Tf_Idf: 0.5849625007211562\n",
      "The word:'angeles' --- The id_word :4 --- Tf_Idf: 1.5849625007211563\n",
      "The word:'los' --- The id_word :5 --- Tf_Idf: 1.5849625007211563\n"
     ]
    }
   ],
   "source": [
    "get_ipython().run_line_magic('time', 'tfidf_model = models.TfidfModel(corpus_doc2bow_vectors, id2word=dictionary, normalize=False)')\n",
    "corpus_tfidf_vectors=tfidf_model[ corpus_doc2bow_vectors ]\n",
    "print(\"----  tfidf_vectors of the three documents:[(id_word,tf), (id_word,tf),....,(id_word,)]  ----\") \n",
    "com=1\n",
    "for doc_vector in corpus_tfidf_vectors :\n",
    "    \n",
    "    print(\"\\n*** The Document number :(\",com,\")***\")\n",
    "    for d in doc_vector:\n",
    "        id_word,Tf_Idf=d\n",
    "        print(f\"The word:\\'{dictionary[id_word]}\\' --- The id_word :{id_word} --- Tf_Idf: {Tf_Idf}\")\n",
    "        \n",
    "    com+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word_Id :0 ---- occures :2 time(s)\n",
      "The word_Id :1 ---- occures :1 time(s)\n"
     ]
    }
   ],
   "source": [
    "query = \"new new times\"\n",
    "query_bow_vector = dictionary.doc2bow(query.lower().split())\n",
    "for (k,j) in query_bow_vector:\n",
    "    print(f\"The word_Id :{k} ---- occures :{j} time(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word_Id :0 ---- TF_IDF :1.1699250014423124\n",
      "The word_Id :1 ---- TF_IDF :0.5849625007211562\n"
     ]
    }
   ],
   "source": [
    "query_tfidf_vector = tfidf_model[query_bow_vector] \n",
    "for (kk,tf_Idf) in query_tfidf_vector:\n",
    "    print(f\"The word_Id :{kk} ---- TF_IDF :{tf_Idf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the query and the document number: 0  is : 0.7745967\n",
      "Similarity between the query and the document number: 1  is : 0.2926428\n",
      "Similarity between the query and the document number: 2  is : 0.112928025\n"
     ]
    }
   ],
   "source": [
    "index_matrix = similarities.SparseMatrixSimilarity(corpus_tfidf_vectors,num_features=6)\n",
    "sims = index_matrix[query_tfidf_vector]\n",
    "for (indexx,sim) in list(enumerate(sims)):\n",
    "    print(\"Similarity between the query and the document number:\",indexx,\" is :\",sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
